{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7823ba79-bca0-4c86-9d5d-d201bfbe6fdf",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730bab2-4d2b-4133-9967-fc9a50178675",
   "metadata": {},
   "source": [
    "### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1de5f23-a8d4-44cb-9639-892d0091df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import Datasets\n",
    "df1 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df2.csv')\n",
    "df3 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a53a76-908a-47a2-a056-fec16389d02c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d68747ec-75e0-459c-8b19-536f4ef34847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18a0d279-a286-4465-996e-7b755773d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout_rate = 0.2\n",
    "epochs = 5\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])\n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment'])\n",
    "\n",
    "# Vocabulary building\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']:\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "counter.update(['<pad>', '<unk>'])  # Add special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens)\n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_rnn = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_rnn = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37dd09-7c0b-4031-a376-8e7a904228d3",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ed6b9-dc7a-4f17-8e53-aaf2ee628987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0176, Accuracy: 0.4977\n",
      "Epoch 2/5, Loss: 1.0114, Accuracy: 0.5003\n",
      "Epoch 3/5, Loss: 1.0355, Accuracy: 0.4728\n",
      "Epoch 4/5, Loss: 1.0223, Accuracy: 0.4877\n",
      "Epoch 5/5, Loss: 1.0221, Accuracy: 0.4949\n",
      "\n",
      "Evaluation using Test Data(df3): \n",
      "Accuracy: 0.987\n",
      "Recall: 0.987\n",
      "F1 Score: 0.9805\n"
     ]
    }
   ],
   "source": [
    "# Define RNN model\n",
    "class modelRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout_rate):\n",
    "        super(modelRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hn = self.rnn(x)\n",
    "        out = self.dropout(hn[-1])  # Take last hidden state\n",
    "        return self.fc(out)\n",
    "\n",
    "# Instantiate RNN model\n",
    "model_rnn = modelRNN(vocab_size=len(vocab), embed_dim=embed_dim, hidden_dim=hidden_dim, \n",
    "                     num_classes=3, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_rnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_rnn:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_rnn)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")\n",
    "\n",
    "# Evaluation on  df3 (test set)\n",
    "model_rnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_rnn:\n",
    "        outputs = model_rnn(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(\"Evaluation using Test Data(df3): \")\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Recall: {round(recall, 4)}\")\n",
    "print(f\"F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d010a-5135-4fcb-bd62-58908b397a55",
   "metadata": {},
   "source": [
    "### Checking Sentiment Count Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22cd519b-1b95-4840-b3b3-5ab2f5ba9973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    0.499917\n",
       "negative    0.333389\n",
       "neutral     0.166694\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['sentiment'].value_counts(normalize=True)\n",
    "# df3['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66e0ece2-a5ce-44ac-87ad-7127b5697a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    0.986957\n",
       "neutral     0.008696\n",
       "negative    0.004348\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4b1a0-59c6-44c1-9db1-fc1f146c09d9",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4573f-63e0-4501-8a2f-87e4ecc4770e",
   "metadata": {},
   "source": [
    "### Evaluating using Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a308d41-5e65-41c8-a869-4eff889df99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[df1 Eval] Accuracy: 0.4999\n",
      "[df1 Eval] Recall: 0.4999\n",
      "[df1 Eval] F1 Score: 0.3332\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on  df1 (training set)\n",
    "model_rnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader_rnn:  \n",
    "        outputs = model_rnn(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"[df1 Eval] Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"[df1 Eval] Recall: {round(recall, 4)}\")\n",
    "print(f\"[df1 Eval] F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730fa70-2b6b-4f5a-aca6-d92831e630c2",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88ce04-1bc7-41f7-8a04-d3c1c9530f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0438, Accuracy: 0.4644\n",
      "Epoch 2/10, Loss: 1.0421, Accuracy: 0.4696\n",
      "Epoch 3/10, Loss: 1.0434, Accuracy: 0.4667\n",
      "Epoch 4/10, Loss: 1.0451, Accuracy: 0.47\n",
      "Epoch 5/10, Loss: 1.0433, Accuracy: 0.4708\n",
      "Epoch 6/10, Loss: 1.0426, Accuracy: 0.4653\n",
      "Epoch 7/10, Loss: 1.0389, Accuracy: 0.4719\n",
      "Epoch 8/10, Loss: 1.0436, Accuracy: 0.4681\n",
      "Epoch 9/10, Loss: 1.0439, Accuracy: 0.473\n",
      "Epoch 10/10, Loss: 1.0452, Accuracy: 0.4733\n",
      "\n",
      "[New HP] Accuracy: 0.788\n",
      "[New HP] Recall: 0.788\n",
      "[New HP] F1 Score: 0.8703\n"
     ]
    }
   ],
   "source": [
    "# New Hyperparameters\n",
    "learning_rate = 0.005   # Increase lr from .001 to .005\n",
    "batch_size = 16         # Smaller batch size\n",
    "embed_dim = 100\n",
    "hidden_dim = 64         # Decrease from 128 to 64\n",
    "num_layers = 2\n",
    "dropout_rate = 0.3      # Increase  dropout_rate\n",
    "epochs = 10             # Increase epochs\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])\n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment'])\n",
    "\n",
    "# Vocabulary building\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']:\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "counter.update(['<pad>', '<unk>'])  # Add special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens)\n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_rnn = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_rnn = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define RNN model\n",
    "class modelRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout_rate):\n",
    "        super(modelRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hn = self.rnn(x)\n",
    "        out = self.dropout(hn[-1])  # Take last hidden state\n",
    "        return self.fc(out)\n",
    "\n",
    "# Instantiate RNN model\n",
    "model_rnn = modelRNN(vocab_size=len(vocab), embed_dim=embed_dim, hidden_dim=hidden_dim, \n",
    "                     num_classes=3, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_rnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_rnn:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_rnn)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")\n",
    "\n",
    "# Evaluation on  df3 (test set)\n",
    "model_rnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_rnn:\n",
    "        outputs = model_rnn(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(f\"[New HP] Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"[New HP] Recall: {round(recall, 4)}\")\n",
    "print(f\"[New HP] F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51846a-77ef-455e-a098-7818121b773c",
   "metadata": {},
   "source": [
    "### Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fec062e-121e-4ddd-abee-28ad8d74c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91e41a-758e-49c0-8b5e-1ea8922c4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 7s 79ms/step - loss: 1.0222 - accuracy: 0.4842 - val_loss: 0.9892 - val_accuracy: 0.4979\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 6s 75ms/step - loss: 0.8415 - accuracy: 0.6137 - val_loss: 0.9004 - val_accuracy: 0.5967\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 6s 76ms/step - loss: 0.5441 - accuracy: 0.7692 - val_loss: 0.9337 - val_accuracy: 0.5946\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 5s 73ms/step - loss: 0.4205 - accuracy: 0.8446 - val_loss: 1.0510 - val_accuracy: 0.6375\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 6s 75ms/step - loss: 0.2038 - accuracy: 0.9250 - val_loss: 1.3067 - val_accuracy: 0.5983\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 1.3067 - accuracy: 0.5983\n",
      "Dataset Size: 25.0% | Accuracy: 0.5983\n",
      "\n",
      "Epoch 1/5\n",
      "150/150 [==============================] - 14s 92ms/step - loss: 0.9672 - accuracy: 0.5372 - val_loss: 0.8567 - val_accuracy: 0.6283\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 13s 87ms/step - loss: 0.7154 - accuracy: 0.7070 - val_loss: 0.8128 - val_accuracy: 0.6592\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 13s 85ms/step - loss: 0.5191 - accuracy: 0.7795 - val_loss: 0.8621 - val_accuracy: 0.6325\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 13s 88ms/step - loss: 0.3804 - accuracy: 0.8493 - val_loss: 1.1538 - val_accuracy: 0.6617\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 14s 91ms/step - loss: 0.2633 - accuracy: 0.8950 - val_loss: 1.2087 - val_accuracy: 0.6679\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 1.2087 - accuracy: 0.6679\n",
      "Dataset Size: 50.0% | Accuracy: 0.6679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset - Using df1\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "\n",
    "# Split data into training and testing\n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['cleaned_reviewText'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['cleaned_reviewText'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# Build RNN model\n",
    "def ds_rnn_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(SimpleRNN(128, activation='relu', return_sequences=False))  # RNN layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate_rnn(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    model = ds_rnn_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate_rnn(train_data, test_data, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f07cc-444a-4f73-b5f4-6db41c9c41eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.5903 - accuracy: 0.9728 - val_loss: 0.1009 - val_accuracy: 0.9891\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 2928.1108 - accuracy: 0.9946 - val_loss: 27.3504 - val_accuracy: 0.9891\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.1325 - val_accuracy: 0.9891\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.1795 - accuracy: 0.9946 - val_loss: 0.2383 - val_accuracy: 0.9891\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.1778 - accuracy: 0.9946 - val_loss: 0.1388 - val_accuracy: 0.9891\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9891\n",
      "Dataset Size: 25.0% | Accuracy: 0.9891\n",
      "\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 2s 99ms/step - loss: 0.6604 - accuracy: 0.9375 - val_loss: 5.2902 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 0.1042 - accuracy: 0.9918 - val_loss: 6.3010 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.1091 - accuracy: 0.9918 - val_loss: 5.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0435 - accuracy: 0.9918 - val_loss: 5.8099 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 6.1565 - val_accuracy: 0.0000e+00\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.1565 - accuracy: 0.0000e+00\n",
      "Dataset Size: 50.0% | Accuracy: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset - Using df3\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')\n",
    "\n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['review_description'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review_description'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# Build RNN model\n",
    "def ds_rnn_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(SimpleRNN(128, activation='relu', return_sequences=False))  # RNN layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate_rnn(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train RNN model\n",
    "    model = ds_rnn_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate_rnn(train_data, test_data, size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
