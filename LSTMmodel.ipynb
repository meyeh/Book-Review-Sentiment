{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3724ead2",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d923b9",
   "metadata": {},
   "source": [
    "### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cc7e230-621a-4863-93e6-c78e24a9a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import Datasets\n",
    "df1 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df2.csv')\n",
    "df3 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8588f7d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b393afdf-e022-46cb-96fe-2e7fbbfed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff5101-2a6d-4dbf-80c5-11c363258868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001   # Learning rate for the optimizer\n",
    "batch_size = 32         # Batch size for DataLoader\n",
    "embed_dim = 100         # Embedding dimension for word vectors\n",
    "hidden_dim = 128        # Hidden dimension for LSTM\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "dropout_rate = 0.2      # Dropout rate for regularization\n",
    "epochs = 5              # Number of training epochs\n",
    "\n",
    "# Define tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])  \n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment']) \n",
    "\n",
    "# Manual token-to-index\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']: \n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Add special tokens\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# Build vocab with special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "# Map tokens to indices - <unk> if not found in vocab\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class \n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens)  \n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_lstm = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_lstm = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a672c78-dcaf-45dd-8a5f-06369261aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.016, Accuracy: 0.4984\n",
      "Epoch 2/5, Loss: 1.01, Accuracy: 0.5027\n",
      "Epoch 3/5, Loss: 1.0073, Accuracy: 0.5049\n",
      "Epoch 4/5, Loss: 1.0027, Accuracy: 0.5067\n",
      "Epoch 5/5, Loss: 1.0011, Accuracy: 0.5074\n",
      "\n",
      "Evaluation using Test Data(df3): \n",
      "Accuracy: 0.9826\n",
      "Recall: 0.9826\n",
      "F1 Score: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM model\n",
    "class modelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout_rate):\n",
    "        super(modelLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                  # (batch_size, seq_len, embed_dim)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)      # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.dropout(hn[-1])               # Take the last hidden state of last LSTM layer\n",
    "        return self.fc(x)                      # (batch_size, num_classes)\n",
    "\n",
    "# Instantiate model\n",
    "model_lstm = modelLSTM(vocab_size=len(vocab), embed_dim=embed_dim, hidden_dim=hidden_dim, num_classes=3, \n",
    "                       num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_lstm.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_lstm:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_lstm)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")\n",
    "\n",
    "# Evaluate the model on the test set df3\n",
    "model_lstm.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_lstm: \n",
    "        outputs = model_lstm(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(\"Evaluation using Test Data(df3): \")\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Recall: {round(recall, 4)}\")\n",
    "print(f\"F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e3d52-8c24-4fe2-9a51-db81f6ee6eed",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9040bb5-6921-4453-a52d-7c3f038c6a89",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17d77-8414-4e32-af00-05090c934cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.0139, Accuracy: 0.4992\n",
      "Epoch 2/10, Loss: 1.0092, Accuracy: 0.5014\n",
      "Epoch 3/10, Loss: 1.006, Accuracy: 0.5058\n",
      "Epoch 4/10, Loss: 0.9985, Accuracy: 0.5089\n",
      "Epoch 5/10, Loss: 0.9915, Accuracy: 0.5121\n",
      "Epoch 6/10, Loss: 0.9871, Accuracy: 0.5145\n",
      "Epoch 7/10, Loss: 0.9774, Accuracy: 0.5268\n",
      "Epoch 8/10, Loss: 0.8127, Accuracy: 0.6622\n",
      "Epoch 9/10, Loss: 0.6302, Accuracy: 0.7573\n",
      "Epoch 10/10, Loss: 0.5345, Accuracy: 0.788\n",
      "\n",
      "[New HP] Accuracy: 0.8228\n",
      "[New HP] Recall: 0.8228\n",
      "[New HP] F1 Score: 0.894\n"
     ]
    }
   ],
   "source": [
    "# New Hyperparameters\n",
    "learning_rate = 0.005  # Increase lr from .001 to .005\n",
    "batch_size = 16        # Smaller batch size\n",
    "embed_dim = 100\n",
    "hidden_dim = 64        # Decrease from 128 to 64\n",
    "num_layers = 2\n",
    "dropout_rate = 0.3     # Increase dropout_rate\n",
    "epochs = 10            # Increase epochs\n",
    "\n",
    "# Define tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])  \n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment']) \n",
    "\n",
    "# Manual token-to-index\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']:  \n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Add special tokens\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# Build vocab with special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "# Manually map tokens to indices\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens)  \n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_lstm = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_lstm = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Define LSTM model\n",
    "class modelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout_rate):\n",
    "        super(modelLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)              \n",
    "        lstm_out, (hn, cn) = self.lstm(x) \n",
    "        x = self.dropout(hn[-1])            \n",
    "        return self.fc(x)                   \n",
    "\n",
    "# Instantiate model\n",
    "model_lstm = modelLSTM(vocab_size=len(vocab), embed_dim=embed_dim, hidden_dim=hidden_dim, num_classes=3, \n",
    "                       num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_lstm.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_lstm:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_lstm)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")\n",
    "\n",
    "# Evaluate the model on the test set df3\n",
    "model_lstm.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_lstm: \n",
    "        outputs = model_lstm(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(f\"[New HP] Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"[New HP] Recall: {round(recall, 4)}\")\n",
    "print(f\"[New HP] F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2fda4b-0861-4a92-a947-aa7b39fa6600",
   "metadata": {},
   "source": [
    "### Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec79d084-1851-4080-8c25-e46b2c50f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bacff-144c-4be9-ab7b-da5f5340b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      "75/75 [==============================] - 14s 174ms/step - loss: 41613952.0000 - accuracy: 0.4846 - val_loss: 1.0238 - val_accuracy: 0.4971\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 13s 168ms/step - loss: 1.0019 - accuracy: 0.4929 - val_loss: 1.0062 - val_accuracy: 0.4971\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 13s 168ms/step - loss: 0.9770 - accuracy: 0.4929 - val_loss: 0.9950 - val_accuracy: 0.4979\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 13s 169ms/step - loss: 0.9332 - accuracy: 0.5142 - val_loss: 0.9731 - val_accuracy: 0.5467\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 13s 168ms/step - loss: 0.8391 - accuracy: 0.6371 - val_loss: 0.9131 - val_accuracy: 0.6058\n",
      "75/75 [==============================] - 3s 38ms/step - loss: 0.9131 - accuracy: 0.6058\n",
      "Dataset Size: 25.0% | Accuracy: 0.6058\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      "150/150 [==============================] - 41s 266ms/step - loss: 16342973.0000 - accuracy: 0.5437 - val_loss: 0.9684 - val_accuracy: 0.5533\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 40s 268ms/step - loss: 1.1485 - accuracy: 0.6966 - val_loss: 0.7959 - val_accuracy: 0.6696\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 39s 258ms/step - loss: 0.7589 - accuracy: 0.7460 - val_loss: 0.7934 - val_accuracy: 0.6721\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 39s 258ms/step - loss: 928947008.0000 - accuracy: 0.6885 - val_loss: 0.8640 - val_accuracy: 0.6417\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 39s 262ms/step - loss: 307824.2500 - accuracy: 0.7370 - val_loss: 0.8636 - val_accuracy: 0.6438\n",
      "75/75 [==============================] - 2s 32ms/step - loss: 0.8636 - accuracy: 0.6438\n",
      "Dataset Size: 50.0% | Accuracy: 0.6438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset - Using df1\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "\n",
    "# Split data into training and testing\n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['cleaned_reviewText'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['cleaned_reviewText'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# LSTM model\n",
    "def ds_lstm_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=False))  # LSTM layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate_lstm(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train LSTM model\n",
    "    model = ds_lstm_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate_lstm(train_data, test_data, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51f95b-edaa-4881-9cce-7df647a51982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 309ms/step - loss: 0.6597 - accuracy: 0.8587 - val_loss: 0.5950 - val_accuracy: 0.9891\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 279ms/step - loss: 0.4735 - accuracy: 0.9946 - val_loss: 0.0712 - val_accuracy: 0.9891\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 1379916779672829952.0000 - accuracy: 0.9946 - val_loss: nan - val_accuracy: 0.0109\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 267ms/step - loss: nan - accuracy: 0.0054 - val_loss: nan - val_accuracy: 0.0109\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 1s 254ms/step - loss: nan - accuracy: 0.0054 - val_loss: nan - val_accuracy: 0.0109\n",
      "6/6 [==============================] - 0s 29ms/step - loss: nan - accuracy: 0.0109\n",
      "Dataset Size: 25.0% | Accuracy: 0.0109\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 5s 268ms/step - loss: 0.9613 - accuracy: 0.9022 - val_loss: 25.2228 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 3s 242ms/step - loss: nan - accuracy: 0.5190 - val_loss: nan - val_accuracy: 0.0109\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 3s 249ms/step - loss: nan - accuracy: 0.0027 - val_loss: nan - val_accuracy: 0.0109\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 3s 263ms/step - loss: nan - accuracy: 0.0027 - val_loss: nan - val_accuracy: 0.0109\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 3s 250ms/step - loss: nan - accuracy: 0.0027 - val_loss: nan - val_accuracy: 0.0109\n",
      "6/6 [==============================] - 0s 29ms/step - loss: nan - accuracy: 0.0109\n",
      "Dataset Size: 50.0% | Accuracy: 0.0109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset - Using df3\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')\n",
    "\n",
    "# Split data into training and testing \n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['review_description'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review_description'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# Build LSTM model\n",
    "def ds_lstm_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(LSTM(128, activation='relu', return_sequences=False))  # LSTM layer\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate_lstm(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train LSTM model\n",
    "    model = ds_lstm_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate_lstm(train_data, test_data, size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
