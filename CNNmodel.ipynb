{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7f9402",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc77a0",
   "metadata": {},
   "source": [
    "### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a2a371c-64b1-44b9-8e67-dfa0f21934e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import Datasets\n",
    "df1 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df2.csv')\n",
    "df3 = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7cd1e",
   "metadata": {},
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c70ec956-b3a5-47e9-b21e-0b0ced6de8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd129d1-102d-4d79-bfc9-b26795a92389",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0242d75-743e-4da6-afb8-4bb1e75e9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001    # Learning rate for the optimizer\n",
    "batch_size = 32          # Batch size for DataLoader\n",
    "embed_dim = 100          # Embedding dimension for word vectors\n",
    "num_filters = 100        # Number of filters in the convolutional layer\n",
    "kernel_size = 3          # Kernel size for the convolutional layer\n",
    "epochs = 5               # Number of training epochs\n",
    "dropout_rate = 0.2       # Dropout rate for regularization - prevent overfitting\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode sentiment columns that contains the labels (pos., neg., neutral)\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])  \n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment']) \n",
    "\n",
    "# Create token-to-index manually\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']:   \n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Add special tokens manually\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# Build vocab with special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "# Manually map tokens to indices - <unk> if not found in vocab\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens) \n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_cnn = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb217f-9d7c-49e0-8d85-8f515037d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9063, Accuracy: 0.5943\n",
      "Epoch 2/5, Loss: 0.7381, Accuracy: 0.6913\n",
      "Epoch 3/5, Loss: 0.6603, Accuracy: 0.7306\n",
      "Epoch 4/5, Loss: 0.5885, Accuracy: 0.761\n",
      "Epoch 5/5, Loss: 0.5155, Accuracy: 0.7977\n",
      "\n",
      "Evaluation using Test Data(df3): \n",
      "Accuracy: 0.8152\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8906\n"
     ]
    }
   ],
   "source": [
    "# Define CNN model\n",
    "class modelCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, num_filters, kernel_size, dropout_rate):\n",
    "        super(modelCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(num_filters, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)          # (batch_size, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv(x))    # (batch_size, num_filters, L_out)\n",
    "        x = self.pool(x).squeeze(2)     # (batch_size, num_filters)\n",
    "        x = self.dropout(x)             # Apply dropout\n",
    "        return self.fc(x)               # (batch_size, num_classes)\n",
    "\n",
    "# Instantiate model\n",
    "model_cnn = modelCNN(vocab_size=len(vocab), embed_dim=embed_dim, num_classes=3, \n",
    "                     num_filters=num_filters, kernel_size=kernel_size, dropout_rate=dropout_rate)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_cnn:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_cnn)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")   # Print Epoch and Loss/Accuracy associated with it\n",
    "\n",
    "# Evaluate the model on the test set df3\n",
    "model_cnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_cnn: \n",
    "        outputs = model_cnn(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(\"Evaluation using Test Data(df3): \")\n",
    "print(f\"Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"Recall: {round(recall, 4)}\")\n",
    "print(f\"F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f8c00-ce25-4c8c-89c8-bd51da0ddb08",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951a6fb-c1e2-464c-947e-df10703824fa",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5f119-541b-42cc-ac0b-889eeaa64ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9643, Accuracy: 0.6016\n",
      "Epoch 2/10, Loss: 0.744, Accuracy: 0.7045\n",
      "Epoch 3/10, Loss: 0.5931, Accuracy: 0.7688\n",
      "Epoch 4/10, Loss: 0.4577, Accuracy: 0.8343\n",
      "Epoch 5/10, Loss: 0.3355, Accuracy: 0.8793\n",
      "Epoch 6/10, Loss: 0.2852, Accuracy: 0.9047\n",
      "Epoch 7/10, Loss: 0.263, Accuracy: 0.9177\n",
      "Epoch 8/10, Loss: 0.2197, Accuracy: 0.9353\n",
      "Epoch 9/10, Loss: 0.2215, Accuracy: 0.9387\n",
      "Epoch 10/10, Loss: 0.2073, Accuracy: 0.9457\n",
      "\n",
      "[New HP] Accuracy: 0.7217\n",
      "[New HP] Recall: 0.7217\n",
      "[New HP] F1 Score: 0.8293\n"
     ]
    }
   ],
   "source": [
    "# New Hyperparameters\n",
    "learning_rate = 0.005   # Increase lr from .001 to .005\n",
    "batch_size = 16         # Smaller batch size\n",
    "embed_dim = 100\n",
    "hidden_dim = 64         # Add hidden dimension\n",
    "num_layers = 2          # Add num layers\n",
    "dropout_rate = 0.3      # Increase dropout_rate\n",
    "epochs = 10             # Increase epochs\n",
    "\n",
    "# Define tokenizer function\n",
    "def tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode sentiment columns that contains the labels (pos., neg., neutral)\n",
    "df1['encoded_label'] = label_encoder.fit_transform(df1['sentiment'])  \n",
    "df3['encoded_label'] = label_encoder.transform(df3['sentiment'])  \n",
    "\n",
    "# Manual token-to-index\n",
    "counter = Counter()\n",
    "for text in df1['cleaned_reviewText']: \n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Add special tokens\n",
    "counter.update(['<pad>', '<unk>'])\n",
    "\n",
    "# Build vocab with special tokens\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "# Manually map tokens to indices - <unk> if not in vocab\n",
    "def encode(tokens):\n",
    "    unk_index = vocab['<unk>']\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Dataset class \n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = encode(tokens)  \n",
    "        return torch.tensor(indices), self.labels[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded = pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>']).long()\n",
    "    return padded, torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = SentimentDataset(df1['cleaned_reviewText'].tolist(), df1['encoded_label'].tolist(), vocab, tokenizer)\n",
    "test_dataset = SentimentDataset(df3['review_description'].tolist(), df3['encoded_label'].tolist(), vocab, tokenizer)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_cnn = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class modelCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, num_filters, kernel_size, dropout_rate):\n",
    "        super(modelCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab['<pad>'])\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(num_filters, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)          # (batch_size, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv(x))    # (batch_size, num_filters, L_out)\n",
    "        x = self.pool(x).squeeze(2)     # (batch_size, num_filters)\n",
    "        x = self.dropout(x)             # Apply dropout\n",
    "        return self.fc(x)               # (batch_size, num_classes)\n",
    "\n",
    "# Instantiate model\n",
    "model_cnn = modelCNN(vocab_size=len(vocab), embed_dim=embed_dim, num_classes=3, \n",
    "                     num_filters=num_filters, kernel_size=kernel_size, dropout_rate=dropout_rate)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader_cnn:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    avg_loss = running_loss / len(train_loader_cnn)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {round(avg_loss, 4)}, Accuracy: {round(accuracy, 4)}\")\n",
    "\n",
    "# Evaluate the model on the test set df3\n",
    "model_cnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_cnn: \n",
    "        outputs = model_cnn(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\")\n",
    "print(f\"[New HP] Accuracy: {round(accuracy, 4)}\")\n",
    "print(f\"[New HP] Recall: {round(recall, 4)}\")\n",
    "print(f\"[New HP] F1 Score: {round(f1, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62cc73-cdaa-45da-827c-d96ba8589122",
   "metadata": {},
   "source": [
    "### Different Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d8ba443-5a79-4bb2-a530-1730d9c96ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c486688-6ff8-4635-a2ff-6172b437f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 1.0088 - accuracy: 0.4808 - val_loss: 0.9849 - val_accuracy: 0.4971\n",
      "Epoch 2/5\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.7275 - accuracy: 0.7000 - val_loss: 0.7392 - val_accuracy: 0.6954\n",
      "Epoch 3/5\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.3436 - accuracy: 0.8750 - val_loss: 0.8145 - val_accuracy: 0.7096\n",
      "Epoch 4/5\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1320 - accuracy: 0.9604 - val_loss: 0.8826 - val_accuracy: 0.7079\n",
      "Epoch 5/5\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.0360 - accuracy: 0.9954 - val_loss: 1.1454 - val_accuracy: 0.7133\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.1454 - accuracy: 0.7133\n",
      "Dataset Size: 25.0% | Accuracy: 0.7133\n",
      "\n",
      "Epoch 1/5\n",
      "150/150 [==============================] - 2s 8ms/step - loss: 0.9185 - accuracy: 0.5676 - val_loss: 0.7279 - val_accuracy: 0.7125\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.5810 - accuracy: 0.7614 - val_loss: 0.6898 - val_accuracy: 0.7221\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.3334 - accuracy: 0.8714 - val_loss: 0.8288 - val_accuracy: 0.6817\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1422 - accuracy: 0.9579 - val_loss: 1.0350 - val_accuracy: 0.7008\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0507 - accuracy: 0.9873 - val_loss: 1.2866 - val_accuracy: 0.7262\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.2866 - accuracy: 0.7262\n",
      "Dataset Size: 50.0% | Accuracy: 0.7262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset - Using df1\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df1.csv')\n",
    "\n",
    "# Split data into training and testing - 80% train, 20% test\n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['cleaned_reviewText'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['cleaned_reviewText'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# CNN model\n",
    "def ds_cnn_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train CNN model\n",
    "    model = ds_cnn_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate(train_data, test_data, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7852b4-6ca6-4c66-8f30-f124d24a77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 54ms/step - loss: 0.2716 - accuracy: 0.9239 - val_loss: 0.0620 - val_accuracy: 0.9891\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0300 - accuracy: 0.9946 - val_loss: 0.0959 - val_accuracy: 0.9891\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0338 - accuracy: 0.9946 - val_loss: 0.1129 - val_accuracy: 0.9891\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 0.0344 - accuracy: 0.9946 - val_loss: 0.1066 - val_accuracy: 0.9891\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 0.0224 - accuracy: 0.9946 - val_loss: 0.0971 - val_accuracy: 0.9891\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9891\n",
      "Dataset Size: 25.0% | Accuracy: 0.9891\n",
      "\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 30ms/step - loss: 0.2619 - accuracy: 0.9103 - val_loss: 12.2804 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1145 - accuracy: 0.9918 - val_loss: 13.6786 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0872 - accuracy: 0.9918 - val_loss: 9.0005 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0594 - accuracy: 0.9918 - val_loss: 5.3719 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.0404 - accuracy: 0.9918 - val_loss: 6.4193 - val_accuracy: 0.0000e+00\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 6.4193 - accuracy: 0.0000e+00\n",
      "Dataset Size: 50.0% | Accuracy: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset - Using df3\n",
    "df_ds = pd.read_csv(r'C:\\Users\\megan\\Desktop\\ML Project\\processed_df3.csv')\n",
    "\n",
    "# Split data into training and testing - 80% train, 20% test\n",
    "train_data, test_data = train_test_split(df_ds, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and padding function\n",
    "def preprocess_data(data, tokenizer=None, max_words=10000, max_len=100):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(data['review_description'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review_description'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Convert labels to integers (sentiment)\n",
    "def encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(labels)\n",
    "\n",
    "# CNN model\n",
    "def ds_cnn_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=input_length))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(train_data, test_data, subset_size):\n",
    "    # Subset the training data\n",
    "    train_subset = train_data.sample(frac=subset_size, random_state=42)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, tokenizer = preprocess_data(train_subset, max_words=10000)\n",
    "    X_test, _ = preprocess_data(test_data, tokenizer=tokenizer, max_words=10000)\n",
    "    \n",
    "    y_train = encode_labels(train_subset['sentiment'])\n",
    "    y_test = encode_labels(test_data['sentiment'])\n",
    "    \n",
    "    # Build and train CNN model\n",
    "    model = ds_cnn_model(input_length=X_train.shape[1], num_classes=len(set(y_train)))\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Dataset Size: {subset_size * 100}% | Accuracy: {round(accuracy, 4)}\\n\")\n",
    "\n",
    "# Experiment with 25% and 50% dataset sizes\n",
    "dataset_sizes = [0.25, 0.5]  # 25% and 50% dataset sizes\n",
    "for size in dataset_sizes:\n",
    "    train_and_evaluate(train_data, test_data, size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
